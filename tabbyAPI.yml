network:
  host: 0.0.0.0
  port: 5000
  disable_auth: true
  send_tracebacks: false
  api_servers: ["OAI"]
logging:
  log_prompt: true
  log_generation_params: true
  log_requests: true
model:
  model_dir: models
  inline_model_loading: true
  use_dummy_models: false
  model_name: llama3s
  use_as_default: ["cache_mode"]
  max_seq_len: 32000
  override_base_seq_len:
  tensor_parallel: false
  gpu_split_auto: true
  autosplit_reserve: [96]
  gpu_split: []
  rope_scale: 1.0
  rope_alpha:
  cache_mode: Q8
  cache_size: 32000
  chunk_size: 2048
  max_batch_size:
  prompt_template:
  num_experts_per_token:
  fasttensors: true
  temperature: 0.6
  top_p: 0.9
draft_model:
  draft_model_dir: models
  draft_model_name:
  draft_rope_scale: 1.0
  draft_rope_alpha:
  draft_cache_mode: FP16
lora:
  lora_dir: loras
  loras:
embeddings:
  embedding_model_dir: models
  embeddings_device: cpu
  embedding_model_name:
sampling:
  override_preset:
developer:
  unsafe_launch: false
  disable_request_streaming: false
  cuda_malloc_backend: false
  uvloop: false
  realtime_process_priority: false
